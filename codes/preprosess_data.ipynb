{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer.functions import caffe\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "from chainer.functions import caffe\n",
    "from chainer import cuda, Function, FunctionSet, gradient_check, Variable, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodecsv as csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/relationships-half.json', 'r') as f:\n",
    "    all_relationships = json.load(f)\n",
    "with open('../data/attributes-half.json', 'r') as f:\n",
    "    all_attributes = json.load(f)\n",
    "with open('../data/objects.json', 'r') as f:\n",
    "    all_objects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../work/relationship_train_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    relationship_train_image_ids=reader.next()\n",
    "    relationship_train_image_ids = map(int,relationship_train_image_ids)\n",
    "    \n",
    "with open('../work/relationship_val_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    relationship_val_image_ids=reader.next()\n",
    "    relationship_val_image_ids = map(int,relationship_val_image_ids)\n",
    "\n",
    "with open('../work/relationship_test_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    relationship_test_image_ids=reader.next()\n",
    "    relationship_test_image_ids = map(int,relationship_test_image_ids)\n",
    "\n",
    "with open('../work/attribute_train_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    attribute_train_image_ids=reader.next()\n",
    "    attribute_train_image_ids = map(int,attribute_train_image_ids)\n",
    "    \n",
    "with open('../work/attribute_val_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    attribute_val_image_ids=reader.next()\n",
    "    attribute_val_image_ids = map(int,attribute_val_image_ids)\n",
    "\n",
    "with open('../work/attribute_test_image_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    attribute_test_image_ids=reader.next()\n",
    "    attribute_test_image_ids = map(int,attribute_test_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_relationships(image_id,all_relationships):\n",
    "    for i in xrange(len(all_relationships)):\n",
    "        image_id_=all_relationships[i][u'id']\n",
    "        if image_id==image_id_:\n",
    "            relationships=all_relationships[i][u'relationships']\n",
    "    return relationships\n",
    "\n",
    "def get_image_attributes(image_id,all_attributes):\n",
    "    for i in xrange(len(all_attributes)):\n",
    "        image_id_=all_objects[i][u'id']\n",
    "        if image_id==image_id_:\n",
    "            attributes=all_attributes[i][u'attributes']\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))\n",
    "gpu_id=-1\n",
    "#Load Caffe Model\n",
    "cnn_model_place=\"../chainer_caption_generation/data/bvlc_googlenet_caffe_chainer.pkl\"\n",
    "with open(cnn_model_place, 'r') as f:\n",
    "    func = pickle.load(f)\n",
    "if gpu_id>= 0:\n",
    "    func.to_gpu()\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38918/38918 [21:25<00:00, 30.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_attributes=[]\n",
    "for image_id in tqdm(attribute_train_image_ids):\n",
    "    image_attributes=get_image_attributes(image_id,all_attributes)\n",
    "    for triple in image_attributes:\n",
    "        triple_id=triple[u'id']\n",
    "        triple_attritbues=triple[u'attributes']\n",
    "        for attribute in triple_attritbues:\n",
    "            train_attributes.append([attribute])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15912\n",
      "2834\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(train_attributes)\n",
    "print len(dictionary)\n",
    "fname=\"../work/attribute_dic_freq.txt\"\n",
    "dictionary.save_as_text(fname, sort_by_word=False)\n",
    "dictionary.filter_extremes(no_below=5, no_above=1.0)\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "index2token = dict((v, k) for k, v in dictionary.token2id.iteritems())\n",
    "ukn_id=len(dictionary.token2id)\n",
    "index2token[ukn_id]='unknown'\n",
    "print len(index2token)\n",
    "\n",
    "# with open('../work/index2attribute.pkl', 'w') as f:\n",
    "#     pickle.dump(index2token,f)\n",
    "# relation2index = dict((v, k) for k, v in index2relation.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_relations=[]\n",
    "# for image_id in tqdm(relationship_train_image_ids):\n",
    "#     image_relationships=get_image_relationships(image_id,all_relationships)\n",
    "#     for triple in image_relationships:\n",
    "#         triple_id=triple[u'id']\n",
    "#         triple_predicate=triple[u'predicate']\n",
    "#         train_relations.append([triple_predicate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(train_relations)\n",
    "print len(dictionary)\n",
    "fname=\"../work/relation_dic_freq.txt\"\n",
    "dictionary.save_as_text(fname, sort_by_word=False)\n",
    "dictionary.filter_extremes(no_below=5, no_above=1.0)\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "index2token = dict((v, k) for k, v in dictionary.token2id.iteritems())\n",
    "ukn_id=len(dictionary.token2id)\n",
    "index2token[ukn_id]='unknown'\n",
    "print len(index2token)\n",
    "\n",
    "# with open('../work/index2relation.pkl', 'w') as f:\n",
    "#     pickle.dump(index2token,f)\n",
    "#relation2index = dict((v, k) for k, v in index2relation.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../work/index2relation.pkl', 'r') as f:\n",
    "    index2relation=pickle.load(f)\n",
    "relation2index = dict((v, k) for k, v in index2relation.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../work/index2attribute.pkl', 'r') as f:\n",
    "    index2attribute=pickle.load(f)\n",
    "attribute2index = dict((v, k) for k, v in index2attribute.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_extract_relation_feature(image_ids):\n",
    "    triple_id2subject_feature={}\n",
    "    triple_id2object_feature={}\n",
    "    triple_id2relation_id={}\n",
    "    for image_id in tqdm(image_ids):\n",
    "        image_relationships=get_image_relationships(image_id,all_relationships)\n",
    "        img_file=\"../data/images/%d.jpg\"%image_id\n",
    "        img=cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        all_img_list=[]\n",
    "        triple_ids_tmp=[]\n",
    "        for triple in image_relationships:\n",
    "            triple_id=triple[u'id']\n",
    "            triple_predicate=triple[u'predicate']\n",
    "\n",
    "            if triple_predicate not in relation2index:\n",
    "                continue\n",
    "            relation_id=relation2index[triple_predicate]\n",
    "\n",
    "            triple_subject=triple[u'subject']\n",
    "            triple_subject_name=triple_subject['name']\n",
    "            triple_subject_x=triple_subject['x']\n",
    "            triple_subject_y=triple_subject['y']\n",
    "            triple_subject_w=triple_subject['w']\n",
    "            triple_subject_h=triple_subject['h']\n",
    "\n",
    "            triple_object=triple[u'object']\n",
    "            triple_object_name=triple_object['name']\n",
    "            triple_object_x=triple_object['x']\n",
    "            triple_object_y=triple_object['y']\n",
    "            triple_object_w=triple_object['w']\n",
    "            triple_object_h=triple_object['h']\n",
    "            \n",
    "            try:\n",
    "                img_sbj=img[triple_subject_y:triple_subject_y+triple_subject_h, triple_subject_x:triple_subject_x+triple_subject_w]\n",
    "                img_sbj=cv2.resize(img_sbj,(224,224))\n",
    "                img_sbj=img_sbj.transpose(2, 0, 1)-MEAN_VALUES\n",
    "                img_obj=img[triple_object_y:triple_object_y+triple_object_h, triple_object_x:triple_object_x+triple_object_w]\n",
    "                img_obj=cv2.resize(img_obj,(224,224))\n",
    "                img_obj=img_obj.transpose(2, 0, 1)-MEAN_VALUES\n",
    "            except Exception as e:\n",
    "                print 'image reading error'\n",
    "                print 'type:' + str(type(e))\n",
    "                print 'args:' + str(e.args)\n",
    "                print 'message:' + e.message.strip()\n",
    "                print 'image_id:'+ str(image_id)\n",
    "                print 'triple_id:'+ str(triple_id)\n",
    "                print \"\"\n",
    "                continue\n",
    "            triple_id2relation_id[triple_id]=relation_id\n",
    "            all_img_list.append(img_sbj)\n",
    "            all_img_list.append(img_obj)\n",
    "            triple_ids_tmp.append(triple_id)\n",
    "        \n",
    "        if len(all_img_list)>0:\n",
    "            x_batch = np.array(all_img_list, dtype=np.float32)\n",
    "            if gpu_id >=0:\n",
    "                x = Variable(cuda.to_gpu(x_batch), volatile=True)\n",
    "            else:\n",
    "                x = Variable(x_batch, volatile=True)\n",
    "            image_feature_chainer, = func(inputs={'data': x}, outputs=['pool5/7x7_s1'],\n",
    "                          disable=['loss1/ave_pool', 'loss2/ave_pool','loss3/classifier'],\n",
    "                          train=False)\n",
    "\n",
    "            shape=image_feature_chainer.data.shape[0:2]\n",
    "            image_feature_xp=image_feature_chainer.data.reshape(shape)\n",
    "            image_feature_np=cuda.to_cpu(image_feature_xp)\n",
    "\n",
    "            for i in xrange(image_feature_np.shape[0]/2):\n",
    "                triple_id=triple_ids_tmp[i]\n",
    "                triple_id2subject_feature[triple_id]=image_feature_np[2*i]\n",
    "                triple_id2object_feature[triple_id]=image_feature_np[2*i+1]\n",
    "\n",
    "    return triple_id2subject_feature, triple_id2object_feature,triple_id2relation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.60it/s]\n"
     ]
    }
   ],
   "source": [
    "image_id=relationship_train_image_ids[279]\n",
    "pre_extract_relation_feature([image_id])\n",
    "triple_id2subject_feature, triple_id2object_feature,triple_id2relation_id=pre_extract_relation_feature([image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_extract_attribute_feature(image_ids):\n",
    "    triple_id2subject_feature={}\n",
    "    triple_id2attribute_ids={}\n",
    "    for image_id in image_ids:\n",
    "        image_objects=get_image_attributes(image_id,all_attributes)\n",
    "        img_file=\"../data/images/%d.jpg\"%image_id\n",
    "        img=cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        all_img_list=[]\n",
    "        triple_ids_tmp=[]\n",
    "        for image_object in image_objects:\n",
    "            object_x=image_object[u'x']\n",
    "            object_y=image_object[u'y']\n",
    "            object_w=image_object[u'w']\n",
    "            object_h=image_object[u'h']\n",
    "            triple_id=image_object[u'id']\n",
    "            triple_attritbues=image_object[u'attributes']\n",
    "            try:\n",
    "                img_sbj=img[object_y:object_y+object_h, object_x:object_x+object_w]\n",
    "                img_sbj=cv2.resize(img_sbj,(224,224))\n",
    "                img_sbj=img_sbj.transpose(2, 0, 1)-MEAN_VALUES\n",
    "            except Exception as e:\n",
    "                print e\n",
    "                continue\n",
    "            \n",
    "            attribute_ids=[]\n",
    "            for attribute in triple_attritbues:\n",
    "                if attribute not in attribute2index:\n",
    "                    continue\n",
    "                attribute_id=attribute2index[attribute]\n",
    "                attribute_ids.append(attribute_id)\n",
    "                \n",
    "            if len(attribute_ids) >0:\n",
    "                triple_id2attribute_ids[triple_id]=attribute_ids\n",
    "                all_img_list.append(img_sbj)\n",
    "                triple_ids_tmp.append(triple_id)\n",
    "            else:\n",
    "                continue\n",
    "        try:\n",
    "            if len(all_img_list)>0:\n",
    "                x_batch = np.array(all_img_list, dtype=np.float32)\n",
    "                if gpu_id >=0:\n",
    "                    x = Variable(cuda.to_gpu(x_batch), volatile=True)\n",
    "                else:\n",
    "                    x = Variable(x_batch, volatile=True)\n",
    "                image_feature_chainer, = func(inputs={'data': x}, outputs=['pool5/7x7_s1'],\n",
    "                              disable=['loss1/ave_pool', 'loss2/ave_pool','loss3/classifier'],\n",
    "                              train=False)\n",
    "\n",
    "                shape=image_feature_chainer.data.shape[0:2]\n",
    "                image_feature_xp=image_feature_chainer.data.reshape(shape)\n",
    "                image_feature_np=cuda.to_cpu(image_feature_xp)\n",
    "                \n",
    "                for i in xrange(image_feature_np.shape[0]):\n",
    "                    triple_id=triple_ids_tmp[i]\n",
    "                    triple_id2subject_feature[triple_id]=image_feature_np[i]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print e\n",
    "            continue\n",
    "                \n",
    "    return triple_id2subject_feature,triple_id2attribute_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_id=attribute_train_image_ids[1]\n",
    "triple_id2subject_feature,triple_id2attribute_ids=pre_extract_attribute_feature([image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8197891: array([ 0.01130778,  0.        ,  0.88767415, ...,  0.08469167,\n",
       "         0.        ,  0.03932191], dtype=float32),\n",
       " 8197892: array([ 0.03707002,  0.        ,  0.64999723, ...,  9.11820221,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 8197900: array([ 1.73119259,  0.        ,  0.        , ...,  1.43419051,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 8197909: array([ 0.49129242,  0.        ,  0.06488496, ...,  0.38823116,\n",
       "         0.        ,  0.05593082], dtype=float32),\n",
       " 8197910: array([ 2.36077738,  0.08352984,  0.33083081, ...,  1.64025569,\n",
       "         0.        ,  0.06471568], dtype=float32),\n",
       " 8197916: array([ 0.55168271,  0.40618455,  0.        , ...,  1.19722152,\n",
       "         0.        ,  0.        ], dtype=float32)}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_id2subject_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8197891: [160],\n",
       " 8197892: [1000],\n",
       " 8197900: [1],\n",
       " 8197909: [732],\n",
       " 8197910: [1723],\n",
       " 8197916: [732]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_id2attribute_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_triple_id2image_id={}\n",
    "for i in xrange(len(all_relationships)):\n",
    "    image_id=all_relationships[i][u'id']\n",
    "    relationships=all_relationships[i][u'relationships']\n",
    "    for relation_triple in relationships:\n",
    "        triple_id=relation_triple[u'id']\n",
    "        rel_triple_id2image_id[triple_id]=image_id\n",
    "with open('../work/rel_triple_id2image_id.pkl', 'w') as f:\n",
    "    pickle.dump(rel_triple_id2image_id,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415741"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rel_triple_id2image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108077/108077 [00:00<00:00, 143176.76it/s]\n"
     ]
    }
   ],
   "source": [
    "attr_triple_id2image_id={}\n",
    "for i in tqdm(xrange(len(all_attributes))):\n",
    "    image_id_=all_attributes[i][u'id']\n",
    "    image_attributes=all_attributes[i][u'attributes']\n",
    "    for triple in image_attributes:\n",
    "        triple_id=triple[u'id']\n",
    "        triple_attritbues=triple[u'attributes']\n",
    "        if len(triple_attritbues) > 0:\n",
    "            attr_triple_id2image_id[triple_id]=image_id_\n",
    "            \n",
    "with open('../work/attr_triple_id2image_id.pkl', 'w') as f:\n",
    "    pickle.dump(attr_triple_id2image_id,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464527"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attr_triple_id2image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
